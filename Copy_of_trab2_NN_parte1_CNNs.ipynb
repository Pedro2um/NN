{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8nBZth2Ohpr"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dbZ1HHIJOWVt",
        "outputId": "0d6890a5-fb8b-46f4-8b5b-3fc5d81162ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pigmorais/NN/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/home/pigmorais/.cache/kagglehub/datasets/alsaniipe/chest-x-ray-image/versions/1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import os\n",
        "# Download latest version\n",
        "\n",
        "ds_path = kagglehub.dataset_download(\"alsaniipe/chest-x-ray-image\")\n",
        "ds_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4B9P4qJ9RGF0"
      },
      "outputs": [],
      "source": [
        "data_path = os.path.join(ds_path,\"Data/\")\n",
        "train_path = os.path.join(data_path, \"train/\")\n",
        "test_path = os.path.join(data_path, \"test/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7nJWjSeDQ0NT"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms.v2 as v2\n",
        "\n",
        "def load_img(path):\n",
        "  # Le a imagem em diversos formatos e garante que a imagem tenha 3 canais\n",
        "  img = Image.open(path).convert('RGB')\n",
        "  # converte para um tensor do pytorch\n",
        "  img = v2.functional.to_image(img)\n",
        "  # garante que seja uma imagem de 8 bits reescalando os valores adequadamente\n",
        "  img = v2.functional.to_dtype(img, dtype=torch.uint8, scale=True) # Remove this line\n",
        "  return img # Return as PIL Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb9m7Li-dgx8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from pathlib import Path # Import Path\n",
        "\n",
        "class ChestXrayDataset(IterableDataset):\n",
        "  def __init__(self, data_path, transforms):\n",
        "    self._data_path = data_path\n",
        "    self._transforms = transforms\n",
        "\n",
        "  def __iter__(self):\n",
        "    class_dirs = sorted([d for d in Path(self._data_path).iterdir() if d.is_dir()]) # Convert to Path\n",
        "    class_iters = [iter(d.iterdir()) for d in class_dirs]\n",
        "\n",
        "    # balanced batch\n",
        "    while True:\n",
        "      batch = []\n",
        "      all_done = True\n",
        "\n",
        "\n",
        "      for class_idx, class_iter in enumerate(class_iters):\n",
        "        class_batch = []\n",
        "        try:\n",
        "          for _ in range(self._batch_size):\n",
        "            path = next(class_iter)\n",
        "            sample = self._preprocess(path)\n",
        "            class_batch.append(sample)\n",
        "          batch.extend(class_batch)\n",
        "          all_done = False\n",
        "        except StopIteration:\n",
        "          continue # class folder ended\n",
        "\n",
        "      if batch:\n",
        "        yield batch\n",
        "      if all_done:\n",
        "        break\n",
        "\n",
        "  def _preprocess(self, path):\n",
        "    img = load_img(path)\n",
        "    return (self._transforms(img), self._get_label_from_path(path))\n",
        "\n",
        "  def _get_label_from_path(self, path):\n",
        "    _path = str(path).lower() # Convert path to string for lower()\n",
        "    if 'covid' in _path: return 0\n",
        "    elif 'normal' in _path: return 1\n",
        "    else: return 2  # pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT3KAynqTbB4"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "train_data = ChestXrayDataset(data_path=train_path, transforms=train_transforms)\n",
        "train_data_loader = DataLoader(dataset=train_data, batch_size=64, num_workers=2)\n",
        "\n",
        "val_data = ChestXrayDataset()\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "test_data = ChestXrayDataset(data_path=test_path, transforms=test_transforms)\n",
        "test_data_loader = DataLoader(dataset=test_data, batch_size=64 num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VwBpBfM0Ni"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_valid_split_idx(batch_size=None, n_classes=3, train_size=0.8):\n",
        "  train = list()\n",
        "  valid = list()\n",
        "  if train_size > 1.0 or train_size <= 0:\n",
        "    train_size = 0.8\n",
        "\n",
        "  for _ in range(n_classes):\n",
        "    i = 0\n",
        "    j = i + (batch_size//n_classes)\n",
        "    idx = list(range(i, j))\n",
        "    np.random.shuffle(idx)\n",
        "    _end = int(train_size * len(idx))\n",
        "    train.extend(idx[:_end])\n",
        "    valid.extend(idx[_end:])\n",
        "\n",
        "  train = np.asarray(train)\n",
        "  valid = np.asarray(valid)\n",
        "\n",
        "  return train, valid\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, checkpoint_dir, state_dict, save_every_nth_epoch=2):\n",
        "  if (epoch + 1) % save_every_nth_epoch == 0:\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('model_checkpoint') and f.endswith('.pth')]\n",
        "    checkpoints = sorted(checkpoints, key=lambda f: os.path.getmtime(os.path.join(checkpoint_dir, f)))\n",
        "\n",
        "    if len(checkpoints) >= 3:\n",
        "      oldest = checkpoints[0]\n",
        "      os.remove(os.path.join(checkpoint_dir, oldest))\n",
        "\n",
        "    new_index = epoch + 1\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'model_checkpoint{new_index}.pth')\n",
        "    torch.save(state_dict, checkpoint_path)\n",
        "\n",
        "def train_model(model=None, optimizer=None, train_data_loader=None, loss_module=None, epochs=200, logging_dir='drive/MyDrive/Experimentos/runs/cnn'):\n",
        "  model.train()\n",
        "  device = next(model.parameters()).device\n",
        "  #writer = SummaryWriter(logging_dir)\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    epoch_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    # OBS: validação por batch ao invés de ser por epoch (pior, sim, mas é um teste válido caso seu dataset seja IMENSO (mundo real) )\n",
        "\n",
        "    for batch in train_data_loader:\n",
        "      # split train and validation images\n",
        "      #inputs, labels = map(list, zip(*batch)) # Remove this line\n",
        "      inputs, labels = zip(*batch) # Unpack the batch into images and labels\n",
        "      # Reshape inputs to flatten the first two dimensions (DataLoader batch and Dataset batch)\n",
        "      inputs = torch.cat(inputs, dim=0).to(device) # Concatenate the tensors in the list along the batch dimension\n",
        "      labels = torch.cat(labels, dim=0).to(device) # Concatenate the labels as well\n",
        "\n",
        "      # inputs = torch.stack(inputs).numpy() # Remove this line\n",
        "      # labels = np.array(labels) # Remove this line\n",
        "\n",
        "      train_idx, val_idx = train_valid_split_idx(len(inputs)) # Use the total number of samples in the combined batch\n",
        "      inputs_train, labels_train, inputs_val, val_labels  = inputs[train_idx], labels[train_idx], inputs[val_idx], labels[val_idx]\n",
        "\n",
        "      ##########################################################################\n",
        "      #training mini-batch\n",
        "      optimizer.zero_grad()\n",
        "      #outputs = model(inputs) # Remove this line\n",
        "      outputs = model(inputs_train) # Use inputs_train\n",
        "      loss = loss_module(outputs, labels_train.long()) # Use labels_train and convert to long\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "      ##########################################################################\n",
        "      # validation mini-batch\n",
        "      with torch.no_grad():\n",
        "        #outputs = model(inputs_val) # Remove this line\n",
        "        outputs = model(inputs_val) # Use inputs_val\n",
        "        loss = loss_module(outputs, val_labels.long()) # Use val_labels and convert to long\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "      save_checkpoint(epoch=epoch, checkpoint_dir='drive/MyDrive/Experimentos/NN/', state_dict=model.state_dict(), save_every_nth_epoch=10)\n",
        "      print(f'epoch {epoch} train loss: {epoch_loss[-1]} val loss {val_loss[-1]}')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    #print(train_imgs.shape) # (153, 1, 3, 224, 224)\n",
        "    #print(valid_imgs.shape) # (39, 1, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1P9RME_CXlq"
      },
      "outputs": [],
      "source": [
        "# feature extractor\n",
        "conv = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 5), # Changed input channels from 1 to 3\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(6, 16, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),   # lembre-se do flatten!\n",
        "    nn.Linear(16 * 53 * 53, 120), # Updated input size to 16 * 53 * 53\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(120, 84)\n",
        ")\n",
        "\n",
        "# rede completa: feature extractor + classifier\n",
        "net = nn.Sequential(\n",
        "    conv,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(84, 3) # Changed output size to 3 for 3 classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O792BCnpCm6r"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_module = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VfFJQGMCvQg",
        "outputId": "b1cddcfe-6e71-4d6c-c212-d4151edd00fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 train loss: 1.1151663064956665 val loss 0.8107205629348755\n",
            "epoch 0 train loss: 0.8001296520233154 val loss 0.32571619749069214\n",
            "epoch 0 train loss: 0.3250850439071655 val loss 0.0562884695827961\n",
            "epoch 0 train loss: 4.853396415710449 val loss 3.096543550491333\n",
            "epoch 0 train loss: 3.110407590866089 val loss 1.6809492111206055\n",
            "epoch 0 train loss: 1.6682136058807373 val loss 0.9250344038009644\n",
            "epoch 0 train loss: 0.9239198565483093 val loss 0.6195053458213806\n",
            "epoch 0 train loss: 0.623062014579773 val loss 0.49903732538223267\n",
            "epoch 0 train loss: 0.4928534924983978 val loss 0.4165429174900055\n",
            "epoch 0 train loss: 1.9989428520202637 val loss 1.8669449090957642\n",
            "epoch 0 train loss: 1.8672266006469727 val loss 1.7214477062225342\n",
            "epoch 0 train loss: 1.6730124950408936 val loss 1.5152573585510254\n",
            "epoch 0 train loss: 1.5205094814300537 val loss 1.3460578918457031\n",
            "epoch 0 train loss: 1.3525410890579224 val loss 1.2316302061080933\n",
            "epoch 0 train loss: 1.2383354902267456 val loss 1.1322050094604492\n",
            "epoch 0 train loss: 1.133509874343872 val loss 1.06526517868042\n",
            "epoch 0 train loss: 1.0626405477523804 val loss 1.013567566871643\n",
            "epoch 0 train loss: 1.0118427276611328 val loss 0.9699007868766785\n",
            "epoch 0 train loss: 0.9711601734161377 val loss 0.9374923706054688\n",
            "epoch 0 train loss: 0.9343240261077881 val loss 0.8995610475540161\n",
            "epoch 0 train loss: 0.9017028212547302 val loss 0.8657761812210083\n",
            "epoch 0 train loss: 0.866982638835907 val loss 0.8271378874778748\n",
            "epoch 0 train loss: 0.8263499140739441 val loss 0.7812371253967285\n",
            "epoch 0 train loss: 0.7860934138298035 val loss 0.7419078946113586\n",
            "epoch 0 train loss: 0.7395288348197937 val loss 0.6831656694412231\n",
            "epoch 0 train loss: 0.6856319308280945 val loss 0.6170931458473206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 1/3 [02:16<04:33, 136.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 train loss: 1.9947657585144043 val loss 1.943920373916626\n",
            "epoch 1 train loss: 1.993330955505371 val loss 1.8124158382415771\n",
            "epoch 1 train loss: 1.8429354429244995 val loss 1.685805082321167\n",
            "epoch 1 train loss: 1.1686718463897705 val loss 1.1699211597442627\n",
            "epoch 1 train loss: 1.169166088104248 val loss 1.1692399978637695\n",
            "epoch 1 train loss: 1.169073462486267 val loss 1.167342185974121\n",
            "epoch 1 train loss: 1.1671391725540161 val loss 1.1644409894943237\n",
            "epoch 1 train loss: 1.1645970344543457 val loss 1.1611661911010742\n",
            "epoch 1 train loss: 1.1615129709243774 val loss 1.157813310623169\n",
            "epoch 1 train loss: 0.8455618619918823 val loss 0.8493484258651733\n",
            "epoch 1 train loss: 0.8470097780227661 val loss 0.8383827209472656\n",
            "epoch 1 train loss: 0.8321794271469116 val loss 0.8169097900390625\n",
            "epoch 1 train loss: 0.8112730383872986 val loss 0.7965784668922424\n",
            "epoch 1 train loss: 0.7793148159980774 val loss 0.7278637886047363\n",
            "epoch 1 train loss: 0.7457111477851868 val loss 0.6842456459999084\n",
            "epoch 1 train loss: 0.7088490724563599 val loss 0.6215857863426208\n",
            "epoch 1 train loss: 0.6153455972671509 val loss 0.5362019538879395\n",
            "epoch 1 train loss: 0.5429579019546509 val loss 0.4364756941795349\n",
            "epoch 1 train loss: 0.44987916946411133 val loss 0.3794914782047272\n",
            "epoch 1 train loss: 0.36121273040771484 val loss 0.3085978031158447\n",
            "epoch 1 train loss: 0.23856781423091888 val loss 0.18579691648483276\n",
            "epoch 1 train loss: 0.2035788744688034 val loss 0.11774930357933044\n",
            "epoch 1 train loss: 0.12600722908973694 val loss 0.10053985565900803\n",
            "epoch 1 train loss: 0.06630714237689972 val loss 0.03171839565038681\n",
            "epoch 1 train loss: 0.057915642857551575 val loss 0.021909723058342934\n",
            "epoch 1 train loss: 0.029158208519220352 val loss 0.013261132873594761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 2/3 [04:31<02:15, 135.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 train loss: 7.531862735748291 val loss 6.596707820892334\n",
            "epoch 2 train loss: 7.127125263214111 val loss 6.170927047729492\n",
            "epoch 2 train loss: 5.237734317779541 val loss 3.980032444000244\n",
            "epoch 2 train loss: 3.6068172454833984 val loss 2.3838143348693848\n",
            "epoch 2 train loss: 2.3112380504608154 val loss 1.6114318370819092\n",
            "epoch 2 train loss: 1.5837903022766113 val loss 1.3019189834594727\n",
            "epoch 2 train loss: 1.297204852104187 val loss 1.203233242034912\n",
            "epoch 2 train loss: 1.198825716972351 val loss 1.1696550846099854\n",
            "epoch 2 train loss: 1.1708351373672485 val loss 1.1541869640350342\n",
            "epoch 2 train loss: 0.9028104543685913 val loss 0.9276166558265686\n",
            "epoch 2 train loss: 0.9291794300079346 val loss 0.9306899905204773\n",
            "epoch 2 train loss: 0.9309723377227783 val loss 0.9296624064445496\n",
            "epoch 2 train loss: 0.9256429076194763 val loss 0.9108467102050781\n",
            "epoch 2 train loss: 0.9183791875839233 val loss 0.9005027413368225\n",
            "epoch 2 train loss: 0.8904810547828674 val loss 0.8554084300994873\n",
            "epoch 2 train loss: 0.8549762964248657 val loss 0.8131614327430725\n",
            "epoch 2 train loss: 0.8156978487968445 val loss 0.7572970986366272\n",
            "epoch 2 train loss: 0.7809139490127563 val loss 0.7295937538146973\n",
            "epoch 2 train loss: 0.723041832447052 val loss 0.6340580582618713\n",
            "epoch 2 train loss: 0.6743808388710022 val loss 0.5739578604698181\n",
            "epoch 2 train loss: 0.6005078554153442 val loss 0.49652791023254395\n",
            "epoch 2 train loss: 0.5100494027137756 val loss 0.44160953164100647\n",
            "epoch 2 train loss: 0.4216320514678955 val loss 0.354523628950119\n",
            "epoch 2 train loss: 0.3344708979129791 val loss 0.2557227611541748\n",
            "epoch 2 train loss: 0.23797355592250824 val loss 0.1905631273984909\n",
            "epoch 2 train loss: 0.1640041172504425 val loss 0.1099608838558197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [06:45<00:00, 135.09s/it]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "train_model(model=net, optimizer=optimizer, train_data_loader=train_data_loader, loss_module=loss_module, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qEoadGzNPCA"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), 'drive/MyDrive/Experimentos/NN/model_initial.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
